import torch
import gc
from psutil import virtual_memory

def free_gpu(verbose: bool = True):
    gc.collect()
    torch.cuda.empty_cache()
    if torch.cuda.is_available():
        torch.cuda.ipc_collect()
    if verbose:
        print("âœ… GPU memory cache cleared.")

def gpu_info():
    if not torch.cuda.is_available():
        print("âš ï¸ CUDA unavailable.")
        return
    print(f"ğŸ–¥ GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸš¦ Memory used: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
    print(f"ğŸ“¦ Reserved memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")

def diagnostic_report(texts, labels=None, batch_size=None):
    print("")
    print("ğŸ“‹ === DIAGNOSTICS ===")
    print(f"ğŸ§¾ Samples in dataset: {len(texts)}")
    if labels:
        print(f"ğŸ· Labels: {set(labels)}")

    avg_len = sum(len(t.split()) for t in texts) / len(texts)
    print(f"ğŸ“ Average text length (words): {avg_len:.1f}")

    bs = batch_size or (16 if torch.cuda.is_available() else 4)
    print(f"âš™ï¸ batch_size: {bs}")

    gpu_info()

    ram = virtual_memory()
    print(f"ğŸ§  RAM available: {ram.available // 1024**2} MB / Total: {ram.total // 1024**2} MB")
    print("âœ… Diagnostics finished.")
    print("")